
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Umsetzung &#8212; From Motion Blur to Motion Flow</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Code" href="Code.html" />
    <link rel="prev" title="Estimating Motion Flow for Blur Removal" href="Model.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">From Motion Blur to Motion Flow</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Introduction.html">
   Einführung
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="RelatedWork.html">
   Deblurring Theorien
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Model.html">
   Estimating Motion Flow for Blur Removal
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Umsetzung
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Code.html">
   Code
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Bibliographie.html">
   Bibliographie
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/Motion_Flow2.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/js2203/motionFlow"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/js2203/motionFlow/issues/new?title=Issue%20on%20page%20%2FMotion_Flow2.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/js2203/motionFlow/master?urlpath=tree/Motion_Flow2.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="umsetzung">
<h1>Umsetzung<a class="headerlink" href="#umsetzung" title="Permalink to this headline">¶</a></h1>
<p><br><strong>Motion-Flow Schätzung und Netzwerk-Design</strong><br><br>
Das Ziel dieses FCN-Netzwerks besteht darin, eine <strong>End-to-End-Mapping</strong> von einem unscharfen Bild auf dessen entsprechende Motion Flow Map zu erreichen. Gegeben sei ein beliebiges RGB-Bild mit der willkürlichen Größe <span class="math notranslate nohighlight">\(P\times Q\)</span>. Das FCN wird dazu verwendet eine Motion Flow-Map zu schätzen <span class="math notranslate nohighlight">\(M=(U,V)\)</span> mit der gleichen Größe wie das Eingabebild, wobei <span class="math notranslate nohighlight">\(U(i,j)\in D_u^+\)</span> and <span class="math notranslate nohighlight">\(V(i,j)\in D_v\)</span>, <span class="math notranslate nohighlight">\(\forall i,j\)</span>.</p>
<p>RGB-Bild mit der willkürlichen Größe <span class="math notranslate nohighlight">\(P\times Q\)</span></p>
<p><span class="math notranslate nohighlight">\(M=(U,V)\)</span> mit der gleichen Größe wie das Eingabebild</p>
<div class="math notranslate nohighlight">
\[U(i,j)\in D_u^+\]</div>
<div class="math notranslate nohighlight">
\[V(i,j)\in D_v, \forall i,j\]</div>
<p><strong>Abbildung 4</strong><img alt="FCN_MotionFlow.PNG" src="_images/FCN_MotionFlow.jpeg" /></p>
<p>Zur Bequemlichkeit, lassen wir  <span class="math notranslate nohighlight">\(D=|D_u^+| + |D_v|\)</span> die Gesamtzahl der Labels bezeichnen
sowohl für <span class="math notranslate nohighlight">\(U\)</span> als auch für <span class="math notranslate nohighlight">\(V\)</span>. Die Netzwerkstruktur ist wie in Abbildung 4 gezeigt, verwendet werden 7 Faltungs-(conv) Layer und 4 Max-Pooling (Pool) Layer sowie 3 uconv-Layer zum Upsampling der Prediction-Map. Uconv bezeichnet  die fraktionierte Faltung, auch bekannt als Deconvolution. Es wird ein kleiner Stride(Schritt) von 1 Pixel für alle Faltungsschichten verwendet. Die uconv-Layer werden mit bilinearer Interpolation initialisiert und werden zum Upsampling der Aktivierungsfunktionen verwendet.</p>
<div class="math notranslate nohighlight">
\[D=|D_u^+| + |D_v|\]</div>
<ul class="simple">
<li><p><strong>7 Faltungs-(conv) Layer</strong></p></li>
<li><p><strong>4 Max-Pooling (Pool) Layer</strong></p></li>
<li><p><strong>3 uconv-Layer</strong> zum Upsampling der Prediction-Map</p></li>
<li><p><strong>Skip-Verbindungen</strong>
-Feature-Map des letzten uconv-Layers (conv7 + uconv2)
ist ein <strong><span class="math notranslate nohighlight">\(P \times Q \times D\)</span>-Tensor</strong></p></li>
<li><p>Stride(Schritt) von 1 Pixel für alle Faltungsschichten</p></li>
<li><p>uconv-Layer werden mit bilinearer Interpolation initialisiert, Upsampling der Aktivierungsfunktionen</p></li>
</ul>
<p>Es werden auch Skip-Verbindungen hinzugefügt, die die Informationen aus verschiedenen Schichten kombinieren, wie in Abbildung 4 gezeigt.
Die Feature-Map des letzten uconv-Layers (conv7 + uconv2)
ist ein <span class="math notranslate nohighlight">\(P \times Q \times D\)</span>-Tensor mit den oberen <span class="math notranslate nohighlight">\(|D_u^+|\)</span>-Slices von Feature-Maps (<span class="math notranslate nohighlight">\(P \times Q \times |D_u^+|\)</span>) entsprechend der Schätzung von <span class="math notranslate nohighlight">\(U\)</span> und den verbleibenden <span class="math notranslate nohighlight">\(|D_v|\)</span>-Slices von Feature-Maps
(<span class="math notranslate nohighlight">\(P \times Q \times |D_v|\)</span>) entsprechend der Schätzung von <span class="math notranslate nohighlight">\(V\)</span>. Zwei
separate Soft-Max-Layer werden jeweils auf diese beiden Teile angewendet, um die Posterior-Wahrscheinlichkeitsschätzung von beiden Kanälen zu erhalten. Sei <span class="math notranslate nohighlight">\(F_{u,i,j}(Y)\)</span> die Wahrscheinlichkeit, dass
der Pixel bei <span class="math notranslate nohighlight">\((i, j)\)</span> eine Bewegung <span class="math notranslate nohighlight">\(u\)</span> entlang der horizontalen
Richtung gemacht hat und <span class="math notranslate nohighlight">\(F_{v,i,j}(Y)\)</span> repräsentiert die Wahrscheinlichkeit, dass der
Pixel bei <span class="math notranslate nohighlight">\((i, j)\)</span> eine Bewegung <span class="math notranslate nohighlight">\(v\)</span> entlang der vertikalen Richtung gemacht hat. Es wird dann die Summe des Kreuzentropieverlustes von beiden Kanälen als die Finale Loss-Function verwendet.</p>
<p><strong>Posterior-Wahrscheinlichkeitsschätzung von beiden Kanälen</strong></p>
<div class="math notranslate nohighlight">
\[F_{u,i,j}(Y)\]</div>
<div class="math notranslate nohighlight">
\[F_{v,i,j}(Y)\]</div>
<p>mit den oberen <span class="math notranslate nohighlight">\(|D_u^+|\)</span>-Slices von Feature-Maps (<span class="math notranslate nohighlight">\(P \times Q \times |D_u^+|\)</span>) entsprechend der Schätzung von <span class="math notranslate nohighlight">\(U\)</span> und den verbleibenden <span class="math notranslate nohighlight">\(|D_v|\)</span>-Slices von Feature-Maps
(<span class="math notranslate nohighlight">\(P \times Q \times |D_v|\)</span>) entsprechend der Schätzung von <span class="math notranslate nohighlight">\(V\)</span>.</p>
<p>Posterior-Wahrscheinlichkeitsschätzung von beiden Kanälen zu erhalten. Sei <span class="math notranslate nohighlight">\(F_{u,i,j}(Y)\)</span> die Wahrscheinlichkeit, dass
der Pixel bei <span class="math notranslate nohighlight">\((i, j)\)</span> eine Bewegung <span class="math notranslate nohighlight">\(u\)</span> entlang der horizontalen
Richtung gemacht hat und <span class="math notranslate nohighlight">\(F_{v,i,j}(Y)\)</span> repräsentiert die Wahrscheinlichkeit, dass der
Pixel bei <span class="math notranslate nohighlight">\((i, j)\)</span> eine Bewegung <span class="math notranslate nohighlight">\(v\)</span> entlang der vertikalen Richtung gemacht hat.</p>
<p>Es wird dann die <strong>Summe des Kreuzentropieverlustes von beiden Kanälen</strong> als die <strong>Finale Loss-Function</strong> verwendet.</p>
<p><img alt="FCN_MotionFlow.PNG" src="_images/loss.PNG" /></p>
<p><span class="math notranslate nohighlight">\(1\)</span> ist eine Indikator Funktion
<br></p>
<p><br><strong>Simulation von Motion-Flow-Maps zur Datengenerierung</strong></p>
<p>Der Kern dieses Abschnitts besteht darin, einen Datensatz zu generieren, der realistische Unschärfemuster auf verschiedenen Bildern für das Training enthält.
Obwohl zufällige Samples sehr unterschiedliche Trainingssamples erzeugen kann, da der realistische Motion Flow einige Eigenschaften wie die stückweise Glätte beibehält.
So ist es das Ziel, eine Simulationsmethode zu schaffen, die Motion Flows erzeugen kann, die die natürlichen Eigenschaften der Bewegung in dem Prozess der Bilderstellung widerspiegelt. Obwohl die Objektbewegung in realen Bildern zu heterogenen Bewegungsunschärfen führen kann, simuliert diese Methode nur den Motion Flow durch Kamerabewegungen fürs Training des FCN. Trotzdem werden Daten, die von dieser Methode erzeugt wurden dem Machine-Learning-Modell auch eine gewisse Handhabung mit Objektbewegung verleihen.</p>
<ul class="simple">
<li><p>einen Datensatz zu generieren, der realistische Unschärfemuster auf verschiedenen Bildern für das Training enthält.</p></li>
</ul>
<ul class="simple">
<li><p>eine Simulationsmethode zu schaffen, die Motion Flows erzeugen kann, die die natürlichen Eigenschaften der Bewegung in dem Prozess der Bilderstellung widerspiegelt.</p></li>
</ul>
<ul class="simple">
<li><p>Obwohl die Objektbewegung in realen Bildern zu heterogenen Bewegungsunschärfen führen kann, simuliert diese Methode nur den Motion Flow durch Kamerabewegungen fürs Training des FCN.</p></li>
</ul>
<ul class="simple">
<li><p>Trotzdem werden Daten, die von dieser Methode erzeugt wurden dem Machine-Learning-Modell auch eine gewisse Handhabung mit Objektbewegung verleihen.</p></li>
</ul>
<p><strong>Abbildung 5</strong><img alt="OBR5.PNG" src="_images/OBR5.PNG" /></p>
<p>Der Einfachheit halber wird ein 3D-Koordinatensystem generiert,
wobei der Ursprung im optischen Zentrum der Kamera, die xy-Ebene
auf die Ebene des Kamerasensors ausgerichtet ist und die z-Achse steht senkrecht zur xy-Ebene, wie in Abbildung 5 gezeigt. Da das
Ziel der Motion Flow auf einem Bildraster ist, wird
der simuliere Motion Flow , der auf ein 2D-Bild projiziert wird.
Der simuliere Motion Flow wird direkt auf ein 2D-Bild projiziert, anstatt
auf die 3D-Bewegungsbahn. In Anbetracht der Unklarheiten
verursacht durch Drehungen um die x- und y-Achse, wird ein
Motion Flow M durch Sampeling von vier additiven Komponenten simuliert:</p>
<ul class="simple">
<li><p>Einfachheit halber wird ein 3D-Koordinatensystem generiert,
wobei der Ursprung im optischen Zentrum der Kamera, die xy-Ebene
auf die Ebene des Kamerasensors ausgerichtet ist</p></li>
</ul>
<ul class="simple">
<li><p>die z-Achse steht senkrecht zur xy-Ebene</p></li>
</ul>
<ul class="simple">
<li><p>Ziel der Motion Flow auf einem Bildraster ist, wird
der simuliere Motion Flow , der auf ein 2D-Bild projiziert wird.
Der simuliere Motion Flow wird direkt auf ein 2D-Bild projiziert, anstatt
auf die 3D-Bewegungsbahn.</p></li>
</ul>
<p>\begin{equation}
M = M_{T_x}+M_{T_y}+M_{T_z}+M_{R_z},
\label{eq:mf_decomp}
\end{equation}</p>
<p>wobei <span class="math notranslate nohighlight">\(M_{T_x}\)</span>, <span class="math notranslate nohighlight">\(M_{T_y}\)</span> und <span class="math notranslate nohighlight">\(M_{T_z}\)</span> die Motion Flows bezeichnet, die mit den Translationen entlang der <span class="math notranslate nohighlight">\(x\)</span>-, <span class="math notranslate nohighlight">\(y\)</span>- und <span class="math notranslate nohighlight">\(z\)</span>-Achse zusammenhängen.
<span class="math notranslate nohighlight">\(M_{R_z}\)</span> repräsentiert die Bewegung aus der Rotation um die z
Achse. Jedes Element wird wie folgt generieren.</p>
<p><strong>Translation entlang der <span class="math notranslate nohighlight">\(x\)</span>- oder <span class="math notranslate nohighlight">\(y\)</span>-Achse</strong> <br>
Als Beispiel beschreiben wir die Erzeugung von <span class="math notranslate nohighlight">\(M_{T_x}\)</span>. Wir tasten zunächst ein zentrales Pixel <span class="math notranslate nohighlight">\(p_{T_x}=(i_{T_x}, j_{T_x})\)</span> auf der Bildebene ab, einen einfachen Bewegungswert <span class="math notranslate nohighlight">\(t_{T_x}\)</span> und einen Beschleunigungskoeffizienten <span class="math notranslate nohighlight">\(r_{T_x}\)</span>. Dann
kann <span class="math notranslate nohighlight">\(M_{T_x}=(U_{T_x}, V_{T_x})\)</span>  wie folgt generiert werden
<span class="math notranslate nohighlight">\(U_{T_x}(i,j) = (i-i_{T_x})r_{T_x} + t_{T_x}, V_{T_x}(i,j) = 0\)</span>. <span class="math notranslate nohighlight">\(M_{T_y}\)</span> kann
auf ähnliche Weise erzeugt werden.</p>
<div class="math notranslate nohighlight">
\[p_{T_x}=(i_{T_x}, j_{T_x})\]</div>
<p>Beschleunigungskoeffizienten <span class="math notranslate nohighlight">\(r_{T_x}\)</span></p>
<p>Bewegungswert <span class="math notranslate nohighlight">\(t_{T_x}\)</span></p>
<p><span class="math notranslate nohighlight">\(M_{T_x}=(U_{T_x}, V_{T_x})\)</span>  wie folgt generiert werden</p>
<div class="math notranslate nohighlight">
\[U_{T_x}(i,j) = (i-i_{T_x})r_{T_x} + t_{T_x}\]</div>
<div class="math notranslate nohighlight">
\[V_{T_x}(i,j) = 0\]</div>
<p><span class="math notranslate nohighlight">\(M_{T_y}\)</span> kann
auf ähnliche Weise erzeugt werden.</p>
<p><img alt="xy_Axis.PNG" src="_images/xy_Axis.PNG" /></p>
<p><strong>Translation entlang der <span class="math notranslate nohighlight">\(z\)</span>-Achse</strong><br>
Die Translation entlang der z-Achse verursacht normalerweise ein radiales Bewegungsunschärfemuster in Richtung des Fluchtpunkts. Indem man den semantischen Kontext ignoriert und ein
einfaches radiales Muster annimmt, kann <span class="math notranslate nohighlight">\(M_{T_z}\)</span> durch <span class="math notranslate nohighlight">\(U_{T_z}(i,j) = t_{T_z} d(i,j)^ζ (i-i_{T_z}), V_{T_z}(i,j) = t_{T_z} d(i,j)^ζ (j-j_{T_z})\)</span> erzeugt werden, wobei
<span class="math notranslate nohighlight">\(p_{T_z}\)</span> einen abgetasteten Fluchtpunkt bezeichnet, <span class="math notranslate nohighlight">\(d(i,j) = \|(i,j)-p_{T_z}\|_2\)</span> ist der Abstand von einem beliebigen Pixel <span class="math notranslate nohighlight">\((i,j)\)</span> zum Fluchtpunkt, ζ und <span class="math notranslate nohighlight">\(t_{T_z}\)</span> werden verwendet, um die Form des radialen Musters zu steuern, welches die Bewegungsgeschwindigkeit widerspiegelt.</p>
<div class="math notranslate nohighlight">
\[ζ , t_{T_z}\]</div>
<p>Form des radialen Musters zu steuern, welches die Bewegungsgeschwindigkeit widerspiegelt.</p>
<div class="math notranslate nohighlight">
\[p_{T_z}\]</div>
<p>einen abgetasteten Fluchtpunkt bezeichnet</p>
<div class="math notranslate nohighlight">
\[d(i,j) = \|(i,j)-p_{T_z}\|_2\]</div>
<p>ist der Abstand von einem beliebigen Pixel <span class="math notranslate nohighlight">\((i,j)\)</span> zum Fluchtpunkt</p>
<div class="math notranslate nohighlight">
\[U_{T_z}(i,j) = t_{T_z} d(i,j)^ζ (i-i_{T_z})\]</div>
<div class="math notranslate nohighlight">
\[V_{T_z}(i,j) = t_{T_z} d(i,j)^ζ (j-j_{T_z})\]</div>
<p><img alt="z_axis.PNG" src="_images/z_axis.PNG" /></p>
<p><strong>Rotation um die z-Achse</strong><br>
Wir tasten zunächst ein Rotationszentrum <span class="math notranslate nohighlight">\(p_{R_z}\)</span> und eine Winkelgeschwindigkeit <span class="math notranslate nohighlight">\(\omega\)</span> ab, wobei <span class="math notranslate nohighlight">\(\omega&gt;0\)</span> die Drehung im Uhrzeigersinn bezeichnet. Sei <span class="math notranslate nohighlight">\(s(i,j)=2d(i,j)tan(\omega/2)\)</span>. Die Bewegungsgröße an jedem Pixel ist <span class="math notranslate nohighlight">\(s(i,j)=2d(i,j)\tan(\omega/2)\)</span>. Indem <span class="math notranslate nohighlight">\(\theta(i,j)=\text{atan}[(i-i_{R_z})/(j-j_{R_z})] \in [-\pi, \pi]\)</span>, Bewegungsvektor am Pixel <span class="math notranslate nohighlight">\((i, j)\)</span> kann als <span class="math notranslate nohighlight">\(\theta(i,j)=\text{atan}[(i-i_{R_z})/(j-j_{R_z})] \in [-\pi, \pi]\)</span> erzeugt werden;  <span class="math notranslate nohighlight">\(U_{R_z}(i,j) = s(i,j) \cos(\theta(i,j)-\pi/2), V_{R_z}(i,j) = s(i,j) \sin(\theta(i,j)-\pi/2)\)</span>.
Wir setzen einheitliche Prioritäten über alle Parameter, die der Motion-Flow-Simulation entsprechen, als <span class="math notranslate nohighlight">\(\text{Uniform}(\alpha, \beta)\)</span>.
Hinweis: Die vier Komponenten werden in kontinuierlicher Domäne simuliert und werden dann als ganze Zahlen diskretisiert.</p>
<p>Rotationszentrum</p>
<div class="math notranslate nohighlight">
\[p_{R_z}\]</div>
<p>Winkelgeschwindigkeit</p>
<div class="math notranslate nohighlight">
\[\omega\]</div>
<p>ab, wobei</p>
<div class="math notranslate nohighlight">
\[\omega&gt;0\]</div>
<p>die Drehung im Uhrzeigersinn</p>
<div class="math notranslate nohighlight">
\[s(i,j)=2d(i,j)tan(\omega/2)\]</div>
<p>Die Bewegungsgröße an jedem Pixel</p>
<p>Bewegungsvektor am Pixel <span class="math notranslate nohighlight">\((i, j)\)</span></p>
<div class="math notranslate nohighlight">
\[\theta(i,j)=\text{atan}[(i-i_{R_z})/(j-j_{R_z})] \in [-\pi, \pi]\]</div>
<div class="math notranslate nohighlight">
\[U_{R_z}(i,j) = s(i,j) \cos(\theta(i,j)-\pi/2)\]</div>
<div class="math notranslate nohighlight">
\[V_{R_z}(i,j) = s(i,j) \sin(\theta(i,j)-\pi/2)\]</div>
<p><img alt="z_rotation.PNG" src="_images/z_rotation.PNG" /></p>
<p><strong>Erstellung von Trainingsdatensätzen</strong><br>
Es wurden 200 Trainingsbilder mit Größen um <span class="math notranslate nohighlight">\(300\times 460\)</span> aus dem Datensatz BSD500
als unser Bildersatz mit scharfen Bildern <span class="math notranslate nohighlight">\(\{X^n\}\)</span> verwendet. Wir simulieren dann unabhängig 10.000 Motionflow-Maps <span class="math notranslate nohighlight">\(\{M^t\}\)</span> mit Reichweiten von <span class="math notranslate nohighlight">\(u_{max}=v_{max}=36\)</span> und weisen jedem  <span class="math notranslate nohighlight">\(X^n\)</span> 50 Motion Flow-Maps zu ohne Duplizierung. Die nicht verunschärften Bilder <span class="math notranslate nohighlight">\(\{X^n\}\)</span> mit
<span class="math notranslate nohighlight">\(U(i,j)=0\)</span> und <span class="math notranslate nohighlight">\(V(i,j)=0\)</span>, <span class="math notranslate nohighlight">\(\forall i,j\)</span> werden zum Training verwendet.
Als Ergebnis haben wir einen Datensatz mit 10.200 Bewegungsunschärfe-MotionFlow-Paaren <span class="math notranslate nohighlight">\(\{Y^t, M^t\}\)</span> für das Training.</p>
<ul class="simple">
<li><p>200 Trainingsbilder mit Größen um <span class="math notranslate nohighlight">\(300\times 460\)</span> Datensatz BSD500</p></li>
</ul>
<ul class="simple">
<li><p>10.000 Motionflow-Maps <span class="math notranslate nohighlight">\(\{M^t\}\)</span> mit Reichweiten von <span class="math notranslate nohighlight">\(u_{max}=v_{max}=36\)</span></p></li>
</ul>
<ul class="simple">
<li><p>jedem  <span class="math notranslate nohighlight">\(X^n\)</span> 50 Motion Flow-Maps zu ohne Duplizierung</p></li>
</ul>
<ul class="simple">
<li><p>10.200 Bewegungsunschärfe-MotionFlow-Paaren</p></li>
</ul>
<p><strong>5. Experiments</strong></p>
<p>Das Modell auf Basis von Caffe implementiert und es wird durch stochastischen Gradientenabstieg mit Impuls und Batch Größe 1 trainiert. Im Training mit dem auf BSD simulierten Datensatz wird eine Lernrate von <span class="math notranslate nohighlight">\(10^{− 9}\)</span> und eine Schrittweite von <span class="math notranslate nohighlight">\(2 × 10^5\)</span> verwendet. Das
Training konvergiert nach <strong>65 Epochen</strong>.</p>
<p><strong>5.1. Datensätze und Bewertungsmetriken</strong><br>
Es werden die Experimente an synthetischen
Datensätzen und Datensätzen von realen Bildern durchgeführt. Da ein Ground-Truth-Motionflow und ein scharfes Bild von einem echten verschwommenen Bild schwer zu erhalten sind. Um eine allgemeine quantitative Bewertung durchzuführen
werden zunächst zwei synthetische Datensätze generiert, die beide 300 unscharfe Bilder enthalten, mit 100 zufälligen scharfen Bildern
aus dem <span class="math notranslate nohighlight">\(BSD500\)</span>-Datensatz und 3 verschiedenen Motion-Flow-Maps
für jedes scharfe Bild. Beachte, dass keine zwei Motion-Flow-Maps gleich sind.
Simuliert wird der Motion-Flow mit <span class="math notranslate nohighlight">\(umax = vmax = 36\)</span>,
dies ist das gleiche wie im Trainingsset. Aus Fairness gegenüber der
Methode noMRF Sun et. al. mit einem kleineren Ausgaberaum generieren wir auch relativ milde Motion-Flows für den zweiten Datensatz mit
<span class="math notranslate nohighlight">\(umax = vmax = 17\)</span>. Diese beiden werden als <span class="math notranslate nohighlight">\(BSD-S\)</span> und
BSD-M bezeichnet. Darüber hinaus bewerten wir die Generalisierungsfähigkeit der vorgeschlagenen Methode anhand von zwei synthetischen
Datensätzen (MC-S und MC-M) mit 60 verschwommenen Bildern, generiert aus 20 scharfen Bildern von Microsoft COCO und
über der Einstellung für die Motion-Flow-Erzeugung.
Bewertungsmetriken Zur Bewertung der Genauigkeit des geschätzten Motion-Flows wird der mittleren quadratischen Fehler
(MSE) der Motion-Flow-Map gemessen. Insbesondere bei einem gegebenen geschätzten Motion-Flow <span class="math notranslate nohighlight">\(M\)</span> und dem Ground-Truth <span class="math notranslate nohighlight">\(\kappa\)</span> ist der <span class="math notranslate nohighlight">\(MSE\)</span>
definiert als <span class="math notranslate nohighlight">\(\frac{1}{2|M|} \!\sum_{i,j}((U(i; j) − \hat U (i; j))^2 + ((V(i; j) −
\hat V (i; j))^2\)</span>, wobei <span class="math notranslate nohighlight">\(|M|\)</span> die Anzahl der Bewegungsvektoren bezeichnet
in <span class="math notranslate nohighlight">\(M\)</span>. Zur Beurteilung der Bildqualität verwenden wir Peak
Signal-Rausch-Verhältnis <span class="math notranslate nohighlight">\((PSNR)\)</span> und struktureller Ähnlichkeitsindex
<span class="math notranslate nohighlight">\((SSIM)\)</span>.</p>
<ul class="simple">
<li><p>2 synthetische Datensätze (300 unscharfe Bilder enthalten, 100 zufällige scharfe Bilder)</p></li>
</ul>
<ul class="simple">
<li><p>100 zufällige scharfe Bilder aus dem  𝐵𝑆𝐷500-Datensatz und 3 verschiedenen Motion-Flow-Maps für jedes scharfe Bild</p></li>
</ul>
<ul class="simple">
<li><p>Simuliert wird der Motion-Flow mit <span class="math notranslate nohighlight">\(umax = vmax = 36\)</span>, BSD-S</p></li>
</ul>
<ul class="simple">
<li><p>zweiten Datensatz mit
<span class="math notranslate nohighlight">\(umax = vmax = 17\)</span>, BSD-M</p></li>
</ul>
<ul class="simple">
<li><p>Datensätzen (MC-S und MC-M) mit 60 verschwommenen Bildern, generiert aus 20 scharfen Bildern von Microsoft COCO und
über der Einstellung für die Motion-Flow-Erzeugung.</p></li>
</ul>
<ul class="simple">
<li><p>Bewertung der Genauigkeit des geschätzten Motion-Flows wird der mittleren quadratischen Fehler
(MSE) der Motion-Flow-Map gemessen</p></li>
</ul>
<p><span class="math notranslate nohighlight">\(MSE\)</span>
definiert als</p>
<div class="math notranslate nohighlight">
\[\frac{1}{2|M|} \!\sum_{i,j}((U(i; j) − \hat U (i; j))^2 + ((V(i; j) −
\hat V (i; j))^2\]</div>
<p>, wobei <span class="math notranslate nohighlight">\(|M|\)</span> die Anzahl der Bewegungsvektoren bezeichnet
in <span class="math notranslate nohighlight">\(M\)</span>.</p>
<p>Zur Beurteilung der Bildqualität verwenden wir Peak
Signal-Noise-Ratio <span class="math notranslate nohighlight">\((PSNR)\)</span> und strukturellen Ähnlichkeitsindex
<span class="math notranslate nohighlight">\((SSIM)\)</span>.</p>
<p>Tabelle 1<img alt="OBR6.PNG" src="_images/OBR6.PNG" /></p>
<p><strong>5.2. Auswertung der Motion-Flow-Schätzung</strong><br>
Wir vergleichen zunächst mit der Methode von Sun <a class="reference external" href="http://et.al">et.al</a>.
(„patchCNN“), die einzige Methode mit verfügbarem Code zum Schätzen des Motion-Flows aus verschwommenen Bildern.
Diese Methode führt Training und Tests an kleinen Bildfeldern durch und verwendet MRF, um die Genauigkeit auf dem gesamten Bild zu verbessern.
Seine Version ohne MRF-Nachbearbeitung
(„noMRF“) wird auch verglichen, wobei die Soft-Max-Ausgabe
direkt verwendet wird um den Motion-Flow wie in unserer Methode zu erhalten. Tabelle 2 zeigt den durchschnittlichen <span class="math notranslate nohighlight">\(MSE\)</span> der geschätzten Motion-Flow-Maps auf allen Bildern in BSD-S und BSD-M. Bemerkenswert ist, dass auch ohne Nachbearbeitung wie <span class="math notranslate nohighlight">\(MRF\)</span> oder
<span class="math notranslate nohighlight">\(CRF\)</span> der Vergleich die hohe Qualität unserer geschätzten Motion-Flow-Maps zeigt. Darüber hinaus kann unsere Methode immer noch einen präzisen Motion-Flow
erzeugen auch bei schwierigeren BSD-S-Datensatz, auf dem die Genauigkeiten der Patch basierenden Methode noMRF Sun et. al. deutlich abnimmt. Wir zeigen auch ein Beispiel für den geschätzten Motion-Flow in Abbildung 6, die
zeigt, dass unser Ergebnis einen reibungslosen Motion-Flow beibehält
sehr ähnlich des Ground Truth, und die Methode von Sun <a class="reference external" href="http://et.al">et.al</a>.
reagiert empfindlicher auf die Bildinhalte. Aus diesem Beispiel,
kann man sehen, <strong>dass die Methode von Sun <a class="reference external" href="http://et.al">et.al</a>. im Allgemeinen die Motionvalues und erzeugte Fehler in der Nähe von
starken Kanten unterschätzt,</strong> vielleicht weil die Verarbeitung auf Patch-Ebene
durch die starken Kanten verwirrt ist und das Unschärfemuster
in einem größeren Bereich ignoriert.</p>
<p>Zum Vergleich mit <strong>anderen blinden Deblurring-Methoden von Xu
und Jia, Xu et al. und Whyte <a class="reference external" href="http://et.al">et.al</a>., die
den Motion-Flow nicht schätzen,</strong> es wird direkt die
Qualität des Bildes ausgewertet, das mit ihrem geschätzten Blur-
Kernel wiederhergestellt wurde. Da die
Nicht-blinde Dekonvolutionsmethode die Wiederherstellungsqualität einschränken kann, bewerten wir die gewonnenen Bilder unter Verwendung des Groundtruth-Motion-Flows als Referenz. Tabelle 1 zeigt die Durchschnitts
Werte auf allen Bildern in jedem Datensatz, was zeigt, dass dieses
Verfahren  deutlich bessere Ergebnisse als die anderen liefert.</p>
<ul class="simple">
<li><p>Training und Tests an kleinen Bildfeldern durch und verwendet MRF, um die Genauigkeit auf dem gesamten Bild zu verbessern.</p></li>
</ul>
<ul class="simple">
<li><p>ohne MRF-Nachbearbeitung
(„noMRF“) wird auch verglichen wobei die Soft-Max-Ausgabe
direkt verwendet wird um den Motion-Flow wie in unserer Methode zu erhalten.</p></li>
</ul>
<ul class="simple">
<li><p>Tabelle 2 zeigt den durchschnittlichen <span class="math notranslate nohighlight">\(MSE\)</span> der geschätzten Motion-Flow-Maps auf allen Bildern in BSD-S und BSD-M.</p></li>
</ul>
<ul class="simple">
<li><p>auch ohne Nachbearbeitung wie <span class="math notranslate nohighlight">\(MRF\)</span> oder
<span class="math notranslate nohighlight">\(CRF\)</span> der Vergleich die hohe Qualität unserer geschätzten Motion-Flow-Maps zeigt.</p></li>
</ul>
<ul class="simple">
<li><p>blinden Deblurring-Methoden von Xu
und Jia, Xu et al. und Whyte <a class="reference external" href="http://et.al">et.al</a>., die
den Motion-Flow nicht schätzen, es wird direkt die
Qualität des Bildes ausgewertet, das mit ihrem geschätzten Blur-
Kernel wiederhergestellt wurde.</p></li>
</ul>
<ul class="simple">
<li><p>Nicht-blinde Dekonvolutionsmethode die Wiederherstellungsqualität einschränken kann, bewerten wir die gewonnenen Bilder unter Verwendung des Groundtruth-Motion-Flows als Referenz.</p></li>
</ul>
<p>Tabelle 2<img alt="OBR8.PNG" src="_images/OBR8.PNG" /></p>
<p><strong>5.3. Bewertung der Generalisierungsfähigkeit</strong><br>
Um die Verallgemeinerungsfähigkeit unseres Ansatzes für unterschiedliche Bilder zu bewerten, verwenden wir die Datensätze auf Basis der Microsoft
COCO (d. h. MC-S und MC-M) zur Evaluierung unseres Modells, das
auf dem Datensatz basierend auf BSD500 trainiert wurde. Tabelle 3 zeigt
die Auswertung und den Vergleich mit dem „patchCNN“.
Die Ergebnisse zeigen, dass unsere Methode stabil Ergebnisse mit hoher Genauigkeit für beide Datensätze produziert. Dieses Experiment
legt nahe, dass die Verallgemeinerungsfähigkeit unseres Ansatzes sehr gut ist.</p>
<ul class="simple">
<li><p>Microsoft
COCO (d. h. MC-S und MC-M)</p></li>
</ul>
<ul class="simple">
<li><p>Experiment
legt nahe, dass die Verallgemeinerungsfähigkeit unseres Ansatzes sehr gut ist.</p></li>
</ul>
<p>Tabelle 3<img alt="OBR10.PNG" src="_images/OBR10.PNG" /></p>
<p><strong>5.4. Laufzeitauswertung</strong><br>
Wir führen einen Laufzeitvergleich mit den relevanten
Motion-Flow-Schätzungsmethoden durch. Durch Ausführen einer Bewegungsflussschätzung für 60 verschwommene Bilder mit Größen von etwa
640 × 480 auf einem PC mit NVIDIA GeForce 980 Grafikkarte und Intel Core i7 CPU. Für die Methode in gilt:
zitiert wird die Laufzeit aus dem Paper. Beachten Sie, dass sowohl die
Methode von Sun <a class="reference external" href="http://et.al">et.al</a>. und diese Methode die GPU verwendet, um die
Berechnungen durchzuführen. Wie in Tabelle 4 gezeigt, braucht die Methode in
sehr lange Zeit aufgrund des iterativen Optimierungsschemas. Unsere
Methode dauert weniger als 10 Sekunden, was effizienter ist
als andere. Die Methode patchCNN benötigt mehr Zeit
weil viele Nachbearbeitungsschritte erforderlich sind.</p>
<ul class="simple">
<li><p>Bewegungsflussschätzung für 60 verschwommene Bilder mit Größen von etwa
640 × 480 auf einem PC mit <strong>NVIDIA GeForce 980 Grafikkarte und Intel Core i7 CPU.</strong></p></li>
</ul>
<ul class="simple">
<li><p>patchCNN benötigt mehr Zeit
weil viele Nachbearbeitungsschritte erforderlich sind.</p></li>
</ul>
<ul class="simple">
<li><p>Sun <a class="reference external" href="http://et.al">et.al</a>. und diese Methode die GPU verwendet, um die
Berechnungen durchzuführen. Wie in Tabelle 4 gezeigt, braucht die Methode in
sehr lange Zeit aufgrund des iterativen Optimierungsschemas.</p></li>
</ul>
<p>Tabelle 4<img alt="OBR11.PNG" src="_images/OBR11.PNG" /></p>
<p><strong>5.5. Auswertung an realen Bildern</strong><br>
Da die Ground-Truth-Bilder von realen verschwommenen Bildern nicht verfügbar sind, präsentieren wir nur die visuelle Bewertung
und Vergleich mit mehreren modernen Methoden für
räumlich variierende Unschärfeentfernung.
Wir vergleichen zunächst die
vorgeschlagene Methode mit der Methode von Sun <a class="reference external" href="http://et.al">et.al</a>. zur Motion-Flow-Schätzung. Vier Beispiele sind in Abbildung 7 dargestellt.
Da das Verfahren von Sun <a class="reference external" href="http://et.al">et.al</a>. auf lokalen Patches durchgeführt wird,
werden ihre Motion-Flow-komponenten oft falsch eingeschätzt, insbesondere wenn das Unschärfemuster in einem kleinen lokalen Bereich subtil oder verwirrend ist, wie beispielsweise in Bereichen mit geringer Beleuchtung oder Texturen. Dank des universellen End-to-End-Mappings können unsere
Methoden natürlichere Ergebnisse mit glattem Fluss erzeugen
und weniger Unordnung. Obwohl wir unser Modell auf Datensätzen trainieren
mit nur sanft variierenden Motion-Flows, verglichen mit
noMRF Sun <a class="reference external" href="http://et.al">et.al</a>. kann unsere Methode bessere Ergebnisse bei Bildern mit
bewegtem Objekt liefern.
<strong>Vergleich mit der Methode  Kim <a class="reference external" href="http://et.al">et.al</a>.</strong> Bei Kim et. al. verwenden sie
ein ähnliches heterogenes Bewegungsunschärfemodell wie unseres und
schätzen auch den Motion-Flow zum Entschärfen. Weil ihr Code
nicht verfügbar ist, führen wir direkt einen Vergleich mit ihren realen Daten durch. Abbildung 8 zeigt die Ergebnisse an einem Beispiel. Verglichen mit den Ergebnissen von Kim und Lee ist spiegelt unser Bewegungsfluss das komplexe Unschärfemuster genauer wider, und unser
wiederhergestelltes Bild enthält mehr Details und weniger Artefakte.</p>
<ul class="simple">
<li><p>Da die Ground-Truth-Bilder von realen verschwommenen Bildern nicht verfügbar sind, präsentieren wir <strong>nur die visuelle Bewertung</strong>
und Vergleich mit mehreren modernen Methoden für
räumlich variierende Unschärfeentfernung.</p></li>
</ul>
<ul class="simple">
<li><p>Da das Verfahren von Sun <a class="reference external" href="http://et.al">et.al</a>. auf lokalen Patches durchgeführt wird,
werden ihre Motion-Flow-komponenten oft falsch eingeschätzt,insbesondere wenn das Unschärfemuster in einem kleinen lokalen Bereich subtil oder verwirrend ist, wie beispielsweise in Bereichen mit geringer Beleuchtung oder Texturen.</p></li>
</ul>
<ul class="simple">
<li><p>universellen End-to-End-Mappings können unsere
Methoden natürlichere Ergebnisse mit glattem Fluss erzeugen
und weniger Unordnung.</p></li>
</ul>
<ul class="simple">
<li><p>noMRF Sun <a class="reference external" href="http://et.al">et.al</a>. kann unsere Methode bessere Ergebnisse bei Bildern mit
bewegtem Objekt liefern.</p></li>
</ul>
<ul class="simple">
<li><p>Abbildung 8 zeigt die Ergebnisse an einem Beispiel. Verglichen mit den Ergebnissen von Kim und Lee ist spiegelt unser Bewegungsfluss das komplexe Unschärfemuster genauer wider, und unser
wiederhergestelltes Bild enthält mehr Details und weniger Artefakte.</p></li>
</ul>
<p>Abbildung 7<img alt="OBR9.PNG" src="_images/OBR9.PNG" /></p>
<p>Abbildung 8<img alt="OBR14.PNG" src="_images/OBR14.PNG" /></p>
<p><strong>Bilder mit Kamerabewegungsunschärfe</strong> Abbildung 9 zeigt ein Beispiel mit Unschärfe, die hauptsächlich durch die Kamerabewegung verursacht wird.
Das unscharfe Bild, das von der ungleichmäßigen Kameraschütteln erzeugt wird.
Die Shake-Deblurring-Methode Whyte <a class="reference external" href="http://et.al">et.al</a>. leidet unter starker Unschärfe, da ihr Modell die Unschärfe ignoriert, die durch große Vorwärtsbewegungen verursacht wird
. Verglichen mit dem Ergebnis von Sun <a class="reference external" href="http://et.al">et.al</a>., liefert unsere
Methode  ein schärferes Ergebnis mit mehr Details und weniger
Artefakte.
<strong>Bilder mit Objektbewegungsunschärfe</strong> Wir evaluieren unsere Methode
auf Bildern mit Objektbewegungsunschärfe. In Abbildung 10
enthält das Ergebnis von Whyte <a class="reference external" href="http://et.al">et.al</a>.  starke Ringing-Artefakte aufgrund der Objektbewegung. Unsere Methode kann mit<br />
starker Unschärfe im Hintergrund umgehen und erzeugt ein natürlicheres
Bild. Wir vergleichen weiter mit dem segmentierungsbasierten
Entschärfeverfahren von Pan <a class="reference external" href="http://et.al">et.al</a>. auf einem Bild mit großem
Skalenunschärfe durch bewegte Objekte auf statischem Hintergrund.
Wie in Abbildung 11 gezeigt, ist das Ergebnis von Sun <a class="reference external" href="http://et.al">et.al</a>. aufgrund der Unterschätzung des Motion-Flows zu glatt. In dem
Ergebnis von Pan <a class="reference external" href="http://et.al">et.al</a>. einige Details aufgrund der
Segmentierungsfehler verloren gehen. Unsere vorgeschlagene Methode kann die</p>
<p>Details auf unscharfem, sich bewegendem Vordergrund wiederherstellen und behält die Schärfe im
Hintergrund wie im Original.</p>
<ul class="simple">
<li><p>Abbildung 9 zeigt ein Beispiel mit Unschärfe, die hauptsächlich durch die Kamerabewegung verursacht wird.</p></li>
</ul>
<ul class="simple">
<li><p>Whyte <a class="reference external" href="http://et.al">et.al</a>. leidet unter starker Unschärfe, da ihr Modell die Unschärfe ignoriert, die durch große Vorwärtsbewegungen verursacht wird</p></li>
</ul>
<ul class="simple">
<li><p>liefert unsere
Methode  ein schärferes Ergebnis mit mehr Details und weniger
Artefakte.</p></li>
</ul>
<ul class="simple">
<li><p>Wir evaluieren unsere Methode
auf Bildern mit Objektbewegungsunschärfe.</p></li>
</ul>
<ul class="simple">
<li><p>In Abbildung 10
enthält das Ergebnis von Whyte <a class="reference external" href="http://et.al">et.al</a>.  starke Ringing-Artefakte aufgrund der Objektbewegung. Unsere Methode kann mit<br />
starker Unschärfe im Hintergrund umgehen und erzeugt ein natürlicheres
Bild.</p></li>
</ul>
<ul class="simple">
<li><p>Wir vergleichen weiter mit dem segmentierungsbasierten
Entschärfeverfahren von Pan <a class="reference external" href="http://et.al">et.al</a>. auf einem Bild mit großem
Skalenunschärfe durch bewegte Objekte auf statischem Hintergrund.</p></li>
</ul>
<ul class="simple">
<li><p>Abbildung 11 gezeigt, ist das Ergebnis von Sun <a class="reference external" href="http://et.al">et.al</a>. aufgrund der Unterschätzung des Motion-Flows zu glatt.einige Details aufgrund der
Segmentierungsfehler verloren gehen. Unsere vorgeschlagene Methode kann die</p></li>
</ul>
<ul class="simple">
<li><p>Details auf unscharfem, sich bewegendem Vordergrund wiederherstellen und behält die Schärfe im
Hintergrund wie im Original.</p></li>
</ul>
<p>Abbildung 9<img alt="OBR12.PNG" src="_images/OBR12.PNG" /></p>
<p>Abbildung 10<img alt="OBR13.PNG" src="_images/OBR13.PNG" /></p>
<p>Abbildung 11<img alt="OBR4.PNG" src="_images/OBR4.PNG" /></p>
<p><strong>6. Fazit</strong><br>
In diesem Papier haben wird ein flexibles und effizientes Deep
lernbasiertes Verfahren zum Schätzen und Entfernen der heterogenen Bewegungsunschärfe vorgestellt. Durch die Darstellung der Heterogenen
Bewegungsunschärfe als pixelweise lineare Bewegungsunschärfe. Die vorgeschlagene
Methode verwendet ein FCN, um eine dichte Motion-Flow-Karte zum Entfernen von Unschärfen zu schätzen
. Außerdem generieren wir automatisch
Trainingsdaten mit simulierten Motion-Flow-Maps für das Training
des FCN. Experimentelle Ergebnisse sowohl von synthetischen als auch realen Daten zeigen die Exzellenz der vorgeschlagenen Methode.</p>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="Model.html" title="previous page">Estimating Motion Flow for Blur Removal</a>
    <a class='right-next' id="next-link" href="Code.html" title="next page">Code</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Gabriel Veiz & Jannik Smidt<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>