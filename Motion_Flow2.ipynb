{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Umsetzung "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>**Motion-Flow Sch√§tzung und Netzwerk-Design**<br><br>\n",
    "Das Ziel dieses FCN-Netzwerks besteht darin, eine **End-to-End-Mapping** von einem unscharfen Bild auf dessen entsprechende Motion Flow Map zu erreichen. Gegeben sei ein beliebiges RGB-Bild mit der willk√ºrlichen Gr√∂√üe $P\\times Q$. Das FCN wird dazu verwendet eine Motion Flow-Map zu sch√§tzen $M=(U,V)$ mit der gleichen Gr√∂√üe wie das Eingabebild, wobei $U(i,j)\\in D_u^+$ and $V(i,j)\\in D_v$, $\\forall i,j$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RGB-Bild mit der willk√ºrlichen Gr√∂√üe $P\\times Q$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$M=(U,V)$ mit der gleichen Gr√∂√üe wie das Eingabebild"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$U(i,j)\\in D_u^+$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$V(i,j)\\in D_v, \\forall i,j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abbildung 4**![FCN_MotionFlow.PNG](./FCN_MotionFlow.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zur Bequemlichkeit, lassen wir  $D=|D_u^+| + |D_v|$ die Gesamtzahl der Labels bezeichnen\n",
    "sowohl f√ºr $U$ als auch f√ºr $V$. Die Netzwerkstruktur ist wie in Abbildung 4 gezeigt, verwendet werden 7 Faltungs-(conv) Layer und 4 Max-Pooling (Pool) Layer sowie 3 uconv-Layer zum Upsampling der Prediction-Map. Uconv bezeichnet  die fraktionierte Faltung, auch bekannt als Deconvolution. Es wird ein kleiner Stride(Schritt) von 1 Pixel f√ºr alle Faltungsschichten verwendet. Die uconv-Layer werden mit bilinearer Interpolation initialisiert und werden zum Upsampling der Aktivierungsfunktionen verwendet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$D=|D_u^+| + |D_v|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **7 Faltungs-(conv) Layer**\n",
    "- **4 Max-Pooling (Pool) Layer**\n",
    "- **3 uconv-Layer** zum Upsampling der Prediction-Map\n",
    "- **Skip-Verbindungen**\n",
    "-Feature-Map des letzten uconv-Layers (conv7 + uconv2)\n",
    "ist ein **$P \\times Q \\times D$-Tensor**\n",
    "- Stride(Schritt) von 1 Pixel f√ºr alle Faltungsschichten\n",
    "- uconv-Layer werden mit bilinearer Interpolation initialisiert, Upsampling der Aktivierungsfunktionen "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es werden auch Skip-Verbindungen hinzugef√ºgt, die die Informationen aus verschiedenen Schichten kombinieren, wie in Abbildung 4 gezeigt.\n",
    "Die Feature-Map des letzten uconv-Layers (conv7 + uconv2)\n",
    "ist ein $P \\times Q \\times D$-Tensor mit den oberen $|D_u^+|$-Slices von Feature-Maps ($P \\times Q \\times |D_u^+|$) entsprechend der Sch√§tzung von $U$ und den verbleibenden $|D_v|$-Slices von Feature-Maps\n",
    "($P \\times Q \\times |D_v|$) entsprechend der Sch√§tzung von $V$. Zwei\n",
    "separate Soft-Max-Layer werden jeweils auf diese beiden Teile angewendet, um die Posterior-Wahrscheinlichkeitssch√§tzung von beiden Kan√§len zu erhalten. Sei $F_{u,i,j}(Y)$ die Wahrscheinlichkeit, dass\n",
    "der Pixel bei $(i, j)$ eine Bewegung $u$ entlang der horizontalen\n",
    "Richtung gemacht hat und $F_{v,i,j}(Y)$ repr√§sentiert die Wahrscheinlichkeit, dass der\n",
    "Pixel bei $(i, j)$ eine Bewegung $v$ entlang der vertikalen Richtung gemacht hat. Es wird dann die Summe des Kreuzentropieverlustes von beiden Kan√§len als die Finale Loss-Function verwendet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Posterior-Wahrscheinlichkeitssch√§tzung von beiden Kan√§len**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$F_{u,i,j}(Y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$F_{v,i,j}(Y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mit den oberen $|D_u^+|$-Slices von Feature-Maps ($P \\times Q \\times |D_u^+|$) entsprechend der Sch√§tzung von $U$ und den verbleibenden $|D_v|$-Slices von Feature-Maps\n",
    "($P \\times Q \\times |D_v|$) entsprechend der Sch√§tzung von $V$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posterior-Wahrscheinlichkeitssch√§tzung von beiden Kan√§len zu erhalten. Sei $F_{u,i,j}(Y)$ die Wahrscheinlichkeit, dass\n",
    "der Pixel bei $(i, j)$ eine Bewegung $u$ entlang der horizontalen\n",
    "Richtung gemacht hat und $F_{v,i,j}(Y)$ repr√§sentiert die Wahrscheinlichkeit, dass der\n",
    "Pixel bei $(i, j)$ eine Bewegung $v$ entlang der vertikalen Richtung gemacht hat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es wird dann die **Summe des Kreuzentropieverlustes von beiden Kan√§len** als die **Finale Loss-Function** verwendet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![FCN_MotionFlow.PNG](./loss.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$1$ ist eine Indikator Funktion\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>**Simulation von Motion-Flow-Maps zur Datengenerierung**\n",
    "\n",
    "Der Kern dieses Abschnitts besteht darin, einen Datensatz zu generieren, der realistische Unsch√§rfemuster auf verschiedenen Bildern f√ºr das Training enth√§lt.\n",
    "Obwohl zuf√§llige Samples sehr unterschiedliche Trainingssamples erzeugen kann, da der realistische Motion Flow einige Eigenschaften wie die st√ºckweise Gl√§tte beibeh√§lt.\n",
    "So ist es das Ziel, eine Simulationsmethode zu schaffen, die Motion Flows erzeugen kann, die die nat√ºrlichen Eigenschaften der Bewegung in dem Prozess der Bilderstellung widerspiegelt. Obwohl die Objektbewegung in realen Bildern zu heterogenen Bewegungsunsch√§rfen f√ºhren kann, simuliert diese Methode nur den Motion Flow durch Kamerabewegungen f√ºrs Training des FCN. Trotzdem werden Daten, die von dieser Methode erzeugt wurden dem Machine-Learning-Modell auch eine gewisse Handhabung mit Objektbewegung verleihen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- einen Datensatz zu generieren, der realistische Unsch√§rfemuster auf verschiedenen Bildern f√ºr das Training enth√§lt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- eine Simulationsmethode zu schaffen, die Motion Flows erzeugen kann, die die nat√ºrlichen Eigenschaften der Bewegung in dem Prozess der Bilderstellung widerspiegelt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Obwohl die Objektbewegung in realen Bildern zu heterogenen Bewegungsunsch√§rfen f√ºhren kann, simuliert diese Methode nur den Motion Flow durch Kamerabewegungen f√ºrs Training des FCN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Trotzdem werden Daten, die von dieser Methode erzeugt wurden dem Machine-Learning-Modell auch eine gewisse Handhabung mit Objektbewegung verleihen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abbildung 5**![OBR5.PNG](./OBR5.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Einfachheit halber wird ein 3D-Koordinatensystem generiert,\n",
    "wobei der Ursprung im optischen Zentrum der Kamera, die xy-Ebene\n",
    "auf die Ebene des Kamerasensors ausgerichtet ist und die z-Achse steht senkrecht zur xy-Ebene, wie in Abbildung 5 gezeigt. Da das\n",
    "Ziel der Motion Flow auf einem Bildraster ist, wird \n",
    "der simuliere Motion Flow , der auf ein 2D-Bild projiziert wird.\n",
    "Der simuliere Motion Flow wird direkt auf ein 2D-Bild projiziert, anstatt\n",
    "auf die 3D-Bewegungsbahn. In Anbetracht der Unklarheiten\n",
    "verursacht durch Drehungen um die x- und y-Achse, wird ein\n",
    "Motion Flow M durch Sampeling von vier additiven Komponenten simuliert:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Einfachheit halber wird ein 3D-Koordinatensystem generiert,\n",
    "wobei der Ursprung im optischen Zentrum der Kamera, die xy-Ebene\n",
    "auf die Ebene des Kamerasensors ausgerichtet ist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- die z-Achse steht senkrecht zur xy-Ebene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ziel der Motion Flow auf einem Bildraster ist, wird \n",
    "der simuliere Motion Flow , der auf ein 2D-Bild projiziert wird.\n",
    "Der simuliere Motion Flow wird direkt auf ein 2D-Bild projiziert, anstatt\n",
    "auf die 3D-Bewegungsbahn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ M = M_{T_x} + M_{T_y} + M_{T_z} + M_{R_z}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wobei $M_{T_x}$, $M_{T_y}$ und $M_{T_z}$ die Motion Flows bezeichnet, die mit den Translationen entlang der $x$-, $y$- und $z$-Achse zusammenh√§ngen.\n",
    "$M_{R_z}$ repr√§sentiert die Bewegung aus der Rotation um die z\n",
    "Achse. Jedes Element wird wie folgt generieren.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Translation entlang der $x$- oder $y$-Achse** <br>\n",
    "Als Beispiel beschreiben wir die Erzeugung von $M_{T_x}$. Wir tasten zun√§chst ein zentrales Pixel $p_{T_x}=(i_{T_x}, j_{T_x})$ auf der Bildebene ab, einen einfachen Bewegungswert $t_{T_x}$ und einen Beschleunigungskoeffizienten $r_{T_x}$. Dann\n",
    "kann $M_{T_x}=(U_{T_x}, V_{T_x})$  wie folgt generiert werden\n",
    "$U_{T_x}(i,j) = (i-i_{T_x})r_{T_x} + t_{T_x}, V_{T_x}(i,j) = 0$. $M_{T_y}$ kann\n",
    "auf √§hnliche Weise erzeugt werden.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p_{T_x}=(i_{T_x}, j_{T_x})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beschleunigungskoeffizienten $r_{T_x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bewegungswert $t_{T_x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$M_{T_x}=(U_{T_x}, V_{T_x})$  wie folgt generiert werden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$U_{T_x}(i,j) = (i-i_{T_x})r_{T_x} + t_{T_x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $$V_{T_x}(i,j) = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$M_{T_y}$ kann\n",
    "auf √§hnliche Weise erzeugt werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![xy_Axis.PNG](./xy_Axis.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Translation entlang der $z$-Achse**<br>\n",
    "Die Translation entlang der z-Achse verursacht normalerweise ein radiales Bewegungsunsch√§rfemuster in Richtung des Fluchtpunkts. Indem man den semantischen Kontext ignoriert und ein\n",
    "einfaches radiales Muster annimmt, kann $M_{T_z}$ durch $U_{T_z}(i,j) = t_{T_z} d(i,j)^Œ∂ (i-i_{T_z}), V_{T_z}(i,j) = t_{T_z} d(i,j)^Œ∂ (j-j_{T_z})$ erzeugt werden, wobei\n",
    "$p_{T_z}$ einen abgetasteten Fluchtpunkt bezeichnet, $d(i,j) = \\|(i,j)-p_{T_z}\\|_2$ ist der Abstand von einem beliebigen Pixel $(i,j)$ zum Fluchtpunkt, Œ∂ und $t_{T_z}$ werden verwendet, um die Form des radialen Musters zu steuern, welches die Bewegungsgeschwindigkeit widerspiegelt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Œ∂ , t_{T_z}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Form des radialen Musters zu steuern, welches die Bewegungsgeschwindigkeit widerspiegelt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p_{T_z}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " einen abgetasteten Fluchtpunkt bezeichnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$d(i,j) = \\|(i,j)-p_{T_z}\\|_2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ist der Abstand von einem beliebigen Pixel $(i,j)$ zum Fluchtpunkt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$U_{T_z}(i,j) = t_{T_z} d(i,j)^Œ∂ (i-i_{T_z})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $$V_{T_z}(i,j) = t_{T_z} d(i,j)^Œ∂ (j-j_{T_z})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![z_axis.PNG](./z_axis.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rotation um die z-Achse**<br>\n",
    "Wir tasten zun√§chst ein Rotationszentrum $p_{R_z}$ und eine Winkelgeschwindigkeit $\\omega$ ab, wobei $\\omega>0$ die Drehung im Uhrzeigersinn bezeichnet. Sei $s(i,j)=2d(i,j)tan(\\omega/2)$. Die Bewegungsgr√∂√üe an jedem Pixel ist $s(i,j)=2d(i,j)\\tan(\\omega/2)$. Indem $\\theta(i,j)=\\text{atan}[(i-i_{R_z})/(j-j_{R_z})] \\in [-\\pi, \\pi]$, Bewegungsvektor am Pixel $(i, j)$ kann als $\\theta(i,j)=\\text{atan}[(i-i_{R_z})/(j-j_{R_z})] \\in [-\\pi, \\pi]$ erzeugt werden;  $U_{R_z}(i,j) = s(i,j) \\cos(\\theta(i,j)-\\pi/2), V_{R_z}(i,j) = s(i,j) \\sin(\\theta(i,j)-\\pi/2)$.\n",
    "Wir setzen einheitliche Priorit√§ten √ºber alle Parameter, die der Motion-Flow-Simulation entsprechen, als $\\text{Uniform}(\\alpha, \\beta)$.\n",
    "Hinweis: Die vier Komponenten werden in kontinuierlicher Dom√§ne simuliert und werden dann als ganze Zahlen diskretisiert.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rotationszentrum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p_{R_z}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Winkelgeschwindigkeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\omega$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ab, wobei"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\omega>0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "die Drehung im Uhrzeigersinn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$s(i,j)=2d(i,j)tan(\\omega/2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Bewegungsgr√∂√üe an jedem Pixel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bewegungsvektor am Pixel $(i, j)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\theta(i,j)=\\text{atan}[(i-i_{R_z})/(j-j_{R_z})] \\in [-\\pi, \\pi]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$U_{R_z}(i,j) = s(i,j) \\cos(\\theta(i,j)-\\pi/2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$V_{R_z}(i,j) = s(i,j) \\sin(\\theta(i,j)-\\pi/2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![z_rotation.PNG](./z_rotation.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Erstellung von Trainingsdatens√§tzen**<br>\n",
    "Es wurden 200 Trainingsbilder mit Gr√∂√üen um $300\\times 460$ aus dem Datensatz BSD500\n",
    "als unser Bildersatz mit scharfen Bildern $\\{X^n\\}$ verwendet. Wir simulieren dann unabh√§ngig 10.000 Motionflow-Maps $\\{M^t\\}$ mit Reichweiten von $u_{max}=v_{max}=36$ und weisen jedem  $X^n$ 50 Motion Flow-Maps zu ohne Duplizierung. Die nicht verunsch√§rften Bilder $\\{X^n\\}$ mit\n",
    "$U(i,j)=0$ und $V(i,j)=0$, $\\forall i,j$ werden zum Training verwendet.\n",
    "Als Ergebnis haben wir einen Datensatz mit 10.200 Bewegungsunsch√§rfe-MotionFlow-Paaren $\\{Y^t, M^t\\}$ f√ºr das Training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 200 Trainingsbilder mit Gr√∂√üen um $300\\times 460$ Datensatz BSD500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 10.000 Motionflow-Maps $\\{M^t\\}$ mit Reichweiten von $u_{max}=v_{max}=36$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- jedem  $X^n$ 50 Motion Flow-Maps zu ohne Duplizierung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 10.200 Bewegungsunsch√§rfe-MotionFlow-Paaren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Experiments**\n",
    "\n",
    "Das Modell auf Basis von Caffe implementiert und es wird durch stochastischen Gradientenabstieg mit Impuls und Batch Gr√∂√üe 1 trainiert. Im Training mit dem auf BSD simulierten Datensatz wird eine Lernrate von $10^{‚àí 9}$ und eine Schrittweite von $2 √ó 10^5$ verwendet. Das\n",
    "Training konvergiert nach **65 Epochen**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.1. Datens√§tze und Bewertungsmetriken**<br>\n",
    "Es werden die Experimente an synthetischen\n",
    "Datens√§tzen und Datens√§tzen von realen Bildern durchgef√ºhrt. Da ein Ground-Truth-Motionflow und ein scharfes Bild von einem echten verschwommenen Bild schwer zu erhalten sind. Um eine allgemeine quantitative Bewertung durchzuf√ºhren\n",
    "werden zun√§chst zwei synthetische Datens√§tze generiert, die beide 300 unscharfe Bilder enthalten, mit 100 zuf√§lligen scharfen Bildern \n",
    "aus dem $BSD500$-Datensatz und 3 verschiedenen Motion-Flow-Maps\n",
    "f√ºr jedes scharfe Bild. Beachte, dass keine zwei Motion-Flow-Maps gleich sind.\n",
    "Simuliert wird der Motion-Flow mit $umax = vmax = 36$,\n",
    "dies ist das gleiche wie im Trainingsset. Aus Fairness gegen√ºber der\n",
    "Methode noMRF Sun et. al. mit einem kleineren Ausgaberaum generieren wir auch relativ milde Motion-Flows f√ºr den zweiten Datensatz mit\n",
    "$umax = vmax = 17$. Diese beiden werden als $BSD-S$ und\n",
    "BSD-M bezeichnet. Dar√ºber hinaus bewerten wir die Generalisierungsf√§higkeit der vorgeschlagenen Methode anhand von zwei synthetischen\n",
    "Datens√§tzen (MC-S und MC-M) mit 60 verschwommenen Bildern, generiert aus 20 scharfen Bildern von Microsoft COCO und\n",
    "√ºber der Einstellung f√ºr die Motion-Flow-Erzeugung.\n",
    "Bewertungsmetriken Zur Bewertung der Genauigkeit des gesch√§tzten Motion-Flows wird der mittleren quadratischen Fehler\n",
    "(MSE) der Motion-Flow-Map gemessen. Insbesondere bei einem gegebenen gesch√§tzten Motion-Flow $M$ und dem Ground-Truth $\\kappa$ ist der $MSE$\n",
    "definiert als $\\frac{1}{2|M|} \\!\\sum_{i,j}((U(i; j) ‚àí \\hat U (i; j))^2 + ((V(i; j) ‚àí\n",
    "\\hat V (i; j))^2$, wobei $|M|$ die Anzahl der Bewegungsvektoren bezeichnet\n",
    "in $M$. Zur Beurteilung der Bildqualit√§t verwenden wir Peak\n",
    "Signal-Rausch-Verh√§ltnis $(PSNR)$ und struktureller √Ñhnlichkeitsindex\n",
    "$(SSIM)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2 synthetische Datens√§tze (300 unscharfe Bilder enthalten, 100 zuf√§llige scharfe Bilder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 100 zuf√§llige scharfe Bilder aus dem  ùêµùëÜùê∑500-Datensatz und 3 verschiedenen Motion-Flow-Maps f√ºr jedes scharfe Bild"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Simuliert wird der Motion-Flow mit $umax = vmax = 36$, BSD-S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- zweiten Datensatz mit\n",
    "$umax = vmax = 17$, BSD-M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Datens√§tzen (MC-S und MC-M) mit 60 verschwommenen Bildern, generiert aus 20 scharfen Bildern von Microsoft COCO und\n",
    "√ºber der Einstellung f√ºr die Motion-Flow-Erzeugung."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bewertung der Genauigkeit des gesch√§tzten Motion-Flows wird der mittleren quadratischen Fehler\n",
    "(MSE) der Motion-Flow-Map gemessen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$MSE$\n",
    "definiert als "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{1}{2|M|} \\!\\sum_{i,j}((U(i; j) ‚àí \\hat U (i; j))^2 + ((V(i; j) ‚àí\n",
    "\\hat V (i; j))^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ", wobei $|M|$ die Anzahl der Bewegungsvektoren bezeichnet\n",
    "in $M$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zur Beurteilung der Bildqualit√§t verwenden wir Peak\n",
    "Signal-Noise-Ratio $(PSNR)$ und strukturellen √Ñhnlichkeitsindex\n",
    "$(SSIM)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabelle 1![OBR6.PNG](./OBR6.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.2. Auswertung der Motion-Flow-Sch√§tzung**<br>\n",
    "Wir vergleichen zun√§chst mit der Methode von Sun et.al.\n",
    "(‚ÄûpatchCNN‚Äú), die einzige Methode mit verf√ºgbarem Code zum Sch√§tzen des Motion-Flows aus verschwommenen Bildern.\n",
    "Diese Methode f√ºhrt Training und Tests an kleinen Bildfeldern durch und verwendet MRF, um die Genauigkeit auf dem gesamten Bild zu verbessern.\n",
    "Seine Version ohne MRF-Nachbearbeitung\n",
    "(‚ÄûnoMRF‚Äú) wird auch verglichen, wobei die Soft-Max-Ausgabe\n",
    "direkt verwendet wird um den Motion-Flow wie in unserer Methode zu erhalten. Tabelle 2 zeigt den durchschnittlichen $MSE$ der gesch√§tzten Motion-Flow-Maps auf allen Bildern in BSD-S und BSD-M. Bemerkenswert ist, dass auch ohne Nachbearbeitung wie $MRF$ oder\n",
    "$CRF$ der Vergleich die hohe Qualit√§t unserer gesch√§tzten Motion-Flow-Maps zeigt. Dar√ºber hinaus kann unsere Methode immer noch einen pr√§zisen Motion-Flow\n",
    "erzeugen auch bei schwierigeren BSD-S-Datensatz, auf dem die Genauigkeiten der Patch basierenden Methode noMRF Sun et. al. deutlich abnimmt. Wir zeigen auch ein Beispiel f√ºr den gesch√§tzten Motion-Flow in Abbildung 6, die\n",
    "zeigt, dass unser Ergebnis einen reibungslosen Motion-Flow beibeh√§lt\n",
    "sehr √§hnlich des Ground Truth, und die Methode von Sun et.al. \n",
    "reagiert empfindlicher auf die Bildinhalte. Aus diesem Beispiel,\n",
    "kann man sehen, **dass die Methode von Sun et.al. im Allgemeinen die Motionvalues und erzeugte Fehler in der N√§he von\n",
    "starken Kanten untersch√§tzt,** vielleicht weil die Verarbeitung auf Patch-Ebene\n",
    "durch die starken Kanten verwirrt ist und das Unsch√§rfemuster\n",
    "in einem gr√∂√üeren Bereich ignoriert.\n",
    "\n",
    "Zum Vergleich mit **anderen blinden Deblurring-Methoden von Xu\n",
    "und Jia, Xu et al. und Whyte et.al., die\n",
    "den Motion-Flow nicht sch√§tzen,** es wird direkt die \n",
    "Qualit√§t des Bildes ausgewertet, das mit ihrem gesch√§tzten Blur-\n",
    "Kernel wiederhergestellt wurde. Da die\n",
    "Nicht-blinde Dekonvolutionsmethode die Wiederherstellungsqualit√§t einschr√§nken kann, bewerten wir die gewonnenen Bilder unter Verwendung des Groundtruth-Motion-Flows als Referenz. Tabelle 1 zeigt die Durchschnitts\n",
    "Werte auf allen Bildern in jedem Datensatz, was zeigt, dass dieses\n",
    "Verfahren  deutlich bessere Ergebnisse als die anderen liefert.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training und Tests an kleinen Bildfeldern durch und verwendet MRF, um die Genauigkeit auf dem gesamten Bild zu verbessern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ohne MRF-Nachbearbeitung\n",
    "(‚ÄûnoMRF‚Äú) wird auch verglichen wobei die Soft-Max-Ausgabe\n",
    "direkt verwendet wird um den Motion-Flow wie in unserer Methode zu erhalten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tabelle 2 zeigt den durchschnittlichen $MSE$ der gesch√§tzten Motion-Flow-Maps auf allen Bildern in BSD-S und BSD-M."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- auch ohne Nachbearbeitung wie $MRF$ oder\n",
    "$CRF$ der Vergleich die hohe Qualit√§t unserer gesch√§tzten Motion-Flow-Maps zeigt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- blinden Deblurring-Methoden von Xu\n",
    "und Jia, Xu et al. und Whyte et.al., die\n",
    "den Motion-Flow nicht sch√§tzen, es wird direkt die \n",
    "Qualit√§t des Bildes ausgewertet, das mit ihrem gesch√§tzten Blur-\n",
    "Kernel wiederhergestellt wurde."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nicht-blinde Dekonvolutionsmethode die Wiederherstellungsqualit√§t einschr√§nken kann, bewerten wir die gewonnenen Bilder unter Verwendung des Groundtruth-Motion-Flows als Referenz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabelle 2![OBR8.PNG](./OBR8.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.3. Bewertung der Generalisierungsf√§higkeit**<br>\n",
    "Um die Verallgemeinerungsf√§higkeit unseres Ansatzes f√ºr unterschiedliche Bilder zu bewerten, verwenden wir die Datens√§tze auf Basis der Microsoft\n",
    "COCO (d. h. MC-S und MC-M) zur Evaluierung unseres Modells, das\n",
    "auf dem Datensatz basierend auf BSD500 trainiert wurde. Tabelle 3 zeigt\n",
    "die Auswertung und den Vergleich mit dem ‚ÄûpatchCNN‚Äú.\n",
    "Die Ergebnisse zeigen, dass unsere Methode stabil Ergebnisse mit hoher Genauigkeit f√ºr beide Datens√§tze produziert. Dieses Experiment\n",
    "legt nahe, dass die Verallgemeinerungsf√§higkeit unseres Ansatzes sehr gut ist.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Microsoft\n",
    "COCO (d. h. MC-S und MC-M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Experiment\n",
    "legt nahe, dass die Verallgemeinerungsf√§higkeit unseres Ansatzes sehr gut ist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabelle 3![OBR10.PNG](./OBR10.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.4. Laufzeitauswertung**<br>\n",
    "Wir f√ºhren einen Laufzeitvergleich mit den relevanten\n",
    "Motion-Flow-Sch√§tzungsmethoden durch. Durch Ausf√ºhren einer Bewegungsflusssch√§tzung f√ºr 60 verschwommene Bilder mit Gr√∂√üen von etwa\n",
    "640 √ó 480 auf einem PC mit NVIDIA GeForce 980 Grafikkarte und Intel Core i7 CPU. F√ºr die Methode in gilt:\n",
    "zitiert wird die Laufzeit aus dem Paper. Beachten Sie, dass sowohl die\n",
    "Methode von Sun et.al. und diese Methode die GPU verwendet, um die\n",
    "Berechnungen durchzuf√ºhren. Wie in Tabelle 4 gezeigt, braucht die Methode in\n",
    "sehr lange Zeit aufgrund des iterativen Optimierungsschemas. Unsere\n",
    "Methode dauert weniger als 10 Sekunden, was effizienter ist\n",
    "als andere. Die Methode patchCNN ben√∂tigt mehr Zeit\n",
    "weil viele Nachbearbeitungsschritte erforderlich sind.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bewegungsflusssch√§tzung f√ºr 60 verschwommene Bilder mit Gr√∂√üen von etwa\n",
    "640 √ó 480 auf einem PC mit **NVIDIA GeForce 980 Grafikkarte und Intel Core i7 CPU.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- patchCNN ben√∂tigt mehr Zeit\n",
    "weil viele Nachbearbeitungsschritte erforderlich sind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sun et.al. und diese Methode die GPU verwendet, um die\n",
    "Berechnungen durchzuf√ºhren. Wie in Tabelle 4 gezeigt, braucht die Methode in\n",
    "sehr lange Zeit aufgrund des iterativen Optimierungsschemas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabelle 4![OBR11.PNG](./OBR11.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.5. Auswertung an realen Bildern**<br>\n",
    "Da die Ground-Truth-Bilder von realen verschwommenen Bildern nicht verf√ºgbar sind, pr√§sentieren wir nur die visuelle Bewertung\n",
    "und Vergleich mit mehreren modernen Methoden f√ºr\n",
    "r√§umlich variierende Unsch√§rfeentfernung. \n",
    "Wir vergleichen zun√§chst die\n",
    "vorgeschlagene Methode mit der Methode von Sun et.al. zur Motion-Flow-Sch√§tzung. Vier Beispiele sind in Abbildung 7 dargestellt.\n",
    "Da das Verfahren von Sun et.al. auf lokalen Patches durchgef√ºhrt wird,\n",
    "werden ihre Motion-Flow-komponenten oft falsch eingesch√§tzt, insbesondere wenn das Unsch√§rfemuster in einem kleinen lokalen Bereich subtil oder verwirrend ist, wie beispielsweise in Bereichen mit geringer Beleuchtung oder Texturen. Dank des universellen End-to-End-Mappings k√∂nnen unsere\n",
    "Methoden nat√ºrlichere Ergebnisse mit glattem Fluss erzeugen\n",
    "und weniger Unordnung. Obwohl wir unser Modell auf Datens√§tzen trainieren\n",
    "mit nur sanft variierenden Motion-Flows, verglichen mit\n",
    "noMRF Sun et.al. kann unsere Methode bessere Ergebnisse bei Bildern mit\n",
    "bewegtem Objekt liefern.\n",
    "**Vergleich mit der Methode  Kim et.al.** Bei Kim et. al. verwenden sie\n",
    "ein √§hnliches heterogenes Bewegungsunsch√§rfemodell wie unseres und\n",
    "sch√§tzen auch den Motion-Flow zum Entsch√§rfen. Weil ihr Code \n",
    "nicht verf√ºgbar ist, f√ºhren wir direkt einen Vergleich mit ihren realen Daten durch. Abbildung 8 zeigt die Ergebnisse an einem Beispiel. Verglichen mit den Ergebnissen von Kim und Lee ist spiegelt unser Bewegungsfluss das komplexe Unsch√§rfemuster genauer wider, und unser\n",
    "wiederhergestelltes Bild enth√§lt mehr Details und weniger Artefakte.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Da die Ground-Truth-Bilder von realen verschwommenen Bildern nicht verf√ºgbar sind, pr√§sentieren wir **nur die visuelle Bewertung**\n",
    "und Vergleich mit mehreren modernen Methoden f√ºr\n",
    "r√§umlich variierende Unsch√§rfeentfernung."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Da das Verfahren von Sun et.al. auf lokalen Patches durchgef√ºhrt wird,\n",
    "werden ihre Motion-Flow-komponenten oft falsch eingesch√§tzt,insbesondere wenn das Unsch√§rfemuster in einem kleinen lokalen Bereich subtil oder verwirrend ist, wie beispielsweise in Bereichen mit geringer Beleuchtung oder Texturen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- universellen End-to-End-Mappings k√∂nnen unsere\n",
    "Methoden nat√ºrlichere Ergebnisse mit glattem Fluss erzeugen\n",
    "und weniger Unordnung."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- noMRF Sun et.al. kann unsere Methode bessere Ergebnisse bei Bildern mit\n",
    "bewegtem Objekt liefern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Abbildung 8 zeigt die Ergebnisse an einem Beispiel. Verglichen mit den Ergebnissen von Kim und Lee ist spiegelt unser Bewegungsfluss das komplexe Unsch√§rfemuster genauer wider, und unser\n",
    "wiederhergestelltes Bild enth√§lt mehr Details und weniger Artefakte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abbildung 7![OBR9.PNG](./OBR9.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abbildung 8![OBR14.PNG](./OBR14.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bilder mit Kamerabewegungsunsch√§rfe** Abbildung 9 zeigt ein Beispiel mit Unsch√§rfe, die haupts√§chlich durch die Kamerabewegung verursacht wird.\n",
    "Das unscharfe Bild, das von der ungleichm√§√üigen Kamerasch√ºtteln erzeugt wird.\n",
    "Die Shake-Deblurring-Methode Whyte et.al. leidet unter starker Unsch√§rfe, da ihr Modell die Unsch√§rfe ignoriert, die durch gro√üe Vorw√§rtsbewegungen verursacht wird\n",
    ". Verglichen mit dem Ergebnis von Sun et.al., liefert unsere\n",
    "Methode  ein sch√§rferes Ergebnis mit mehr Details und weniger\n",
    "Artefakte.\n",
    "**Bilder mit Objektbewegungsunsch√§rfe** Wir evaluieren unsere Methode\n",
    "auf Bildern mit Objektbewegungsunsch√§rfe. In Abbildung 10\n",
    "enth√§lt das Ergebnis von Whyte et.al.  starke Ringing-Artefakte aufgrund der Objektbewegung. Unsere Methode kann mit  \n",
    "starker Unsch√§rfe im Hintergrund umgehen und erzeugt ein nat√ºrlicheres\n",
    "Bild. Wir vergleichen weiter mit dem segmentierungsbasierten\n",
    "Entsch√§rfeverfahren von Pan et.al. auf einem Bild mit gro√üem\n",
    "Skalenunsch√§rfe durch bewegte Objekte auf statischem Hintergrund.\n",
    "Wie in Abbildung 11 gezeigt, ist das Ergebnis von Sun et.al. aufgrund der Untersch√§tzung des Motion-Flows zu glatt. In dem\n",
    "Ergebnis von Pan et.al. einige Details aufgrund der\n",
    "Segmentierungsfehler verloren gehen. Unsere vorgeschlagene Methode kann die\n",
    "\n",
    "Details auf unscharfem, sich bewegendem Vordergrund wiederherstellen und beh√§lt die Sch√§rfe im\n",
    "Hintergrund wie im Original.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Abbildung 9 zeigt ein Beispiel mit Unsch√§rfe, die haupts√§chlich durch die Kamerabewegung verursacht wird."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Whyte et.al. leidet unter starker Unsch√§rfe, da ihr Modell die Unsch√§rfe ignoriert, die durch gro√üe Vorw√§rtsbewegungen verursacht wird"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- liefert unsere\n",
    "Methode  ein sch√§rferes Ergebnis mit mehr Details und weniger\n",
    "Artefakte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Wir evaluieren unsere Methode\n",
    "auf Bildern mit Objektbewegungsunsch√§rfe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In Abbildung 10\n",
    "enth√§lt das Ergebnis von Whyte et.al.  starke Ringing-Artefakte aufgrund der Objektbewegung. Unsere Methode kann mit  \n",
    "starker Unsch√§rfe im Hintergrund umgehen und erzeugt ein nat√ºrlicheres\n",
    "Bild."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Wir vergleichen weiter mit dem segmentierungsbasierten\n",
    "Entsch√§rfeverfahren von Pan et.al. auf einem Bild mit gro√üem\n",
    "Skalenunsch√§rfe durch bewegte Objekte auf statischem Hintergrund."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Abbildung 11 gezeigt, ist das Ergebnis von Sun et.al. aufgrund der Untersch√§tzung des Motion-Flows zu glatt.einige Details aufgrund der\n",
    "Segmentierungsfehler verloren gehen. Unsere vorgeschlagene Methode kann die\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Details auf unscharfem, sich bewegendem Vordergrund wiederherstellen und beh√§lt die Sch√§rfe im\n",
    "Hintergrund wie im Original."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abbildung 9![OBR12.PNG](./OBR12.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abbildung 10![OBR13.PNG](./OBR13.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abbildung 11![OBR4.PNG](./OBR4.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Fazit**<br>\n",
    "In diesem Papier haben wird ein flexibles und effizientes Deep\n",
    "lernbasiertes Verfahren zum Sch√§tzen und Entfernen der heterogenen Bewegungsunsch√§rfe vorgestellt. Durch die Darstellung der Heterogenen\n",
    "Bewegungsunsch√§rfe als pixelweise lineare Bewegungsunsch√§rfe. Die vorgeschlagene\n",
    "Methode verwendet ein FCN, um eine dichte Motion-Flow-Karte zum Entfernen von Unsch√§rfen zu sch√§tzen\n",
    ". Au√üerdem generieren wir automatisch\n",
    "Trainingsdaten mit simulierten Motion-Flow-Maps f√ºr das Training\n",
    "des FCN. Experimentelle Ergebnisse sowohl von synthetischen als auch realen Daten zeigen die Exzellenz der vorgeschlagenen Methode.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
