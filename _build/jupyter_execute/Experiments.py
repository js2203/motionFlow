#!/usr/bin/env python
# coding: utf-8

# # Experiments

# **5. Experiments**
# 
# Das Modell auf Basis von Caffe implementiert und es wird durch stochastischen Gradientenabstieg mit Impuls und Batch Gr√∂√üe 1 trainiert. Im Training mit dem auf BSD simulierten Datensatz wird eine Lernrate von $10^{‚àí 9}$ und eine Schrittweite von $2 √ó 10^5$ verwendet. Das
# Training konvergiert nach **65 Epochen**.
# 

# **5.1. Datens√§tze und Bewertungsmetriken**<br>
# Es werden die Experimente an synthetischen
# Datens√§tzen und Datens√§tzen von realen Bildern durchgef√ºhrt. Da ein Ground-Truth-Motionflow und ein scharfes Bild von einem echten verschwommenen Bild schwer zu erhalten sind. Um eine allgemeine quantitative Bewertung durchzuf√ºhren
# werden zun√§chst zwei synthetische Datens√§tze generiert, die beide 300 unscharfe Bilder enthalten, mit 100 zuf√§lligen scharfen Bildern 
# aus dem $BSD500$-Datensatz und 3 verschiedenen Motion-Flow-Maps
# f√ºr jedes scharfe Bild. Beachte, dass keine zwei Motion-Flow-Maps gleich sind.
# Simuliert wird der Motion-Flow mit $umax = vmax = 36$,
# dies ist das gleiche wie im Trainingsset. Aus Fairness gegen√ºber der
# Methode noMRF Sun et. al. mit einem kleineren Ausgaberaum generieren wir auch relativ milde Motion-Flows f√ºr den zweiten Datensatz mit
# $umax = vmax = 17$. Diese beiden werden als $BSD-S$ und
# BSD-M bezeichnet. Dar√ºber hinaus bewerten wir die Generalisierungsf√§higkeit der vorgeschlagenen Methode anhand von zwei synthetischen
# Datens√§tzen (MC-S und MC-M) mit 60 verschwommenen Bildern, generiert aus 20 scharfen Bildern von Microsoft COCO und
# √ºber der Einstellung f√ºr die Motion-Flow-Erzeugung.
# Bewertungsmetriken Zur Bewertung der Genauigkeit des gesch√§tzten Motion-Flows wird der mittleren quadratischen Fehler
# (MSE) der Motion-Flow-Map gemessen. Insbesondere bei einem gegebenen gesch√§tzten Motion-Flow $M$ und dem Ground-Truth $\kappa$ ist der $MSE$
# definiert als $\frac{1}{2|M|} \!\sum_{i,j}((U(i; j) ‚àí \hat U (i; j))^2 + ((V(i; j) ‚àí
# \hat V (i; j))^2$, wobei $|M|$ die Anzahl der Bewegungsvektoren bezeichnet
# in $M$. Zur Beurteilung der Bildqualit√§t verwenden wir Peak
# Signal-Rausch-Verh√§ltnis $(PSNR)$ und struktureller √Ñhnlichkeitsindex
# $(SSIM)$.
# 

# **Text in Stichpunkten:**

# - 2 synthetische Datens√§tze (300 unscharfe Bilder enthalten, 100 zuf√§llige scharfe Bilder)

# - F√ºr die 100 zuf√§lligen scharfen Bilder aus dem  ùêµùëÜùê∑500-Datensatz wurden pro Bild 3 verschiedene Motion-Flow-Maps simuliert

# - Simuliert wird der Motion-Flow im ersten Datensatz mit $u_{max}=v_{max}=36$, BSD-S

# - Der zweite Datensatz mit
# $u_{max}=v_{max}=17$, BSD-M

# - Datens√§tze (MC-S und MC-M) mit 60 verschwommenen Bildern, diese wurden generiert aus 20 scharfen Bildern von Microsoft COCO 

# - F√ºr die Bewertung der Genauigkeit des gesch√§tzten Motion-Flows wird der mittlere quadratische Fehler
# (MSE) der Motion-Flow-Map gemessen

# $MSE$
# definiert als:

# $$\frac{1}{2|M|} \!\sum_{i,j}((U(i; j) ‚àí \hat U (i; j))^2 + ((V(i; j) ‚àí
# \hat V (i; j))^2$$

# - $|M|$ bezeichnet die Anzahl der Bewegungsvektoren 
# in $M$.

# - Zur Beurteilung der Bildqualit√§t verwenden wir Peak
# Signal-Noise-Ratio $(PSNR)$, Verh√§ltnis zwischen maximalen Signal und st√∂rendem Rauschen, und strukturellen √Ñhnlichkeitsindex
# $(SSIM)$, der die √Ñhnlichkeit von 2 Bildern misst

# Tabelle 1![OBR6.PNG](./OBR6.PNG)

# **5.2. Auswertung der Motion-Flow-Sch√§tzung**<br>
# Wir vergleichen zun√§chst mit der Methode von Sun et.al.
# (‚ÄûpatchCNN‚Äú), die einzige Methode mit verf√ºgbarem Code zum Sch√§tzen des Motion-Flows aus verschwommenen Bildern.
# Diese Methode f√ºhrt Training und Tests an kleinen Bildfeldern durch und verwendet MRF, um die Genauigkeit auf dem gesamten Bild zu verbessern.
# Seine Version ohne MRF-Nachbearbeitung
# (‚ÄûnoMRF‚Äú) wird auch verglichen, wobei die Soft-Max-Ausgabe
# direkt verwendet wird um den Motion-Flow wie in unserer Methode zu erhalten. Tabelle 2 zeigt den durchschnittlichen $MSE$ der gesch√§tzten Motion-Flow-Maps auf allen Bildern in BSD-S und BSD-M. Bemerkenswert ist, dass auch ohne Nachbearbeitung wie $MRF$ oder
# $CRF$ der Vergleich die hohe Qualit√§t unserer gesch√§tzten Motion-Flow-Maps zeigt. Dar√ºber hinaus kann unsere Methode immer noch einen pr√§zisen Motion-Flow
# erzeugen auch bei schwierigeren BSD-S-Datensatz, auf dem die Genauigkeiten der Patch basierenden Methode noMRF Sun et. al. deutlich abnimmt. Wir zeigen auch ein Beispiel f√ºr den gesch√§tzten Motion-Flow in Abbildung 6, die
# zeigt, dass unser Ergebnis einen reibungslosen Motion-Flow beibeh√§lt
# sehr √§hnlich des Ground Truth, und die Methode von Sun et.al. 
# reagiert empfindlicher auf die Bildinhalte. Aus diesem Beispiel,
# kann man sehen, **dass die Methode von Sun et.al. im Allgemeinen die Motionvalues und erzeugte Fehler in der N√§he von
# starken Kanten untersch√§tzt,** vielleicht weil die Verarbeitung auf Patch-Ebene
# durch die starken Kanten verwirrt ist und das Unsch√§rfemuster
# in einem gr√∂√üeren Bereich ignoriert.
# 
# Zum Vergleich mit **anderen blinden Deblurring-Methoden von Xu
# und Jia, Xu et al. und Whyte et.al., die
# den Motion-Flow nicht sch√§tzen,** es wird direkt die 
# Qualit√§t des Bildes ausgewertet, das mit ihrem gesch√§tzten Blur-
# Kernel wiederhergestellt wurde. Da die
# Nicht-blinde Dekonvolutionsmethode die Wiederherstellungsqualit√§t einschr√§nken kann, bewerten wir die gewonnenen Bilder unter Verwendung des Groundtruth-Motion-Flows als Referenz. Tabelle 1 zeigt die Durchschnitts
# Werte auf allen Bildern in jedem Datensatz, was zeigt, dass dieses
# Verfahren  deutlich bessere Ergebnisse als die anderen liefert.
# 

# **Text in Stichpunkten:**

# - Verglichen wird mit der Sun et. al. Methode, die MRF(Marov Random Fields) verwendet

# - Sun ohne MRF-Nachbearbeitung
# (‚ÄûnoMRF‚Äú) wird auch verglichen wobei die Soft-Max-Ausgabe
# direkt verwendet wird um den Motion-Flow wie in unserer Methode zu erhalten.

# - Tabelle 2 zeigt den durchschnittlichen $MSE$ der gesch√§tzten Motion-Flow-Maps auf allen Bildern in BSD-S und BSD-M.

# - auch ohne Nachbearbeitung wie $MRF$ oder
# $CRF$ zeigt der Vergleich die hohe Qualit√§t unserer gesch√§tzten Motion-Flow-Maps.

# - F√ºr die blinden Deblurring-Methoden von Xu
# und Jia, Xu et al. und Whyte et.al., die
# den Motion-Flow nicht sch√§tzen, wird direkt die 
# Qualit√§t des Bildes ausgewertet, das mit ihrem gesch√§tzten Blur-
# Kernel wiederhergestellt wurde.

# - Da Nicht-blinde Deconvolution-Methode die Wiederherstellungsqualit√§t einschr√§nken kann, bewerten wir die gewonnenen Bilder unter Verwendung des Groundtruth-Motion-Flows als Referenz.

# Tabelle 2![OBR8.PNG](./OBR8.PNG)

# **5.3. Bewertung der Generalisierungsf√§higkeit**<br>
# Um die Verallgemeinerungsf√§higkeit unseres Ansatzes f√ºr unterschiedliche Bilder zu bewerten, verwenden wir die Datens√§tze auf Basis der Microsoft
# COCO (d. h. MC-S und MC-M) zur Evaluierung unseres Modells, das
# auf dem Datensatz basierend auf BSD500 trainiert wurde. Tabelle 3 zeigt
# die Auswertung und den Vergleich mit dem ‚ÄûpatchCNN‚Äú.
# Die Ergebnisse zeigen, dass unsere Methode stabil Ergebnisse mit hoher Genauigkeit f√ºr beide Datens√§tze produziert. Dieses Experiment
# legt nahe, dass die Verallgemeinerungsf√§higkeit unseres Ansatzes sehr gut ist.
# 

# **Text in Stichpunkten:**

# - Microsoft
# COCO (d. h. MC-S und MC-M)

# - Experiment
# legt nahe, dass die Verallgemeinerungsf√§higkeit unseres Ansatzes sehr gut ist.

# Tabelle 3![OBR10.PNG](./OBR10.PNG)

# **5.4. Laufzeitauswertung**<br>
# Wir f√ºhren einen Laufzeitvergleich mit den relevanten
# Motion-Flow-Sch√§tzungsmethoden durch. Durch Ausf√ºhren einer Bewegungsflusssch√§tzung f√ºr 60 verschwommene Bilder mit Gr√∂√üen von etwa
# 640 √ó 480 auf einem PC mit NVIDIA GeForce 980 Grafikkarte und Intel Core i7 CPU. F√ºr die Methode in gilt:
# zitiert wird die Laufzeit aus dem Paper. Beachten Sie, dass sowohl die
# Methode von Sun et.al. und diese Methode die GPU verwendet, um die
# Berechnungen durchzuf√ºhren. Wie in Tabelle 4 gezeigt, braucht die Methode in
# sehr lange Zeit aufgrund des iterativen Optimierungsschemas. Unsere
# Methode dauert weniger als 10 Sekunden, was effizienter ist
# als andere. Die Methode patchCNN ben√∂tigt mehr Zeit
# weil viele Nachbearbeitungsschritte erforderlich sind.
# 

# **Text in Stichpunkten:**

# - Bewegungsflusssch√§tzung f√ºr 60 verschwommene Bilder mit Gr√∂√üen von etwa
# 640 √ó 480 auf einem PC mit **NVIDIA GeForce 980 Grafikkarte und Intel Core i7 CPU.**

# - patchCNN(Sun Methode mit MRF) ben√∂tigt mehr Zeit
# weil viele Nachbearbeitungsschritte erforderlich sind.

# - Da Sun et.al. und unsere Methode die GPU verwendet, um die
# Berechnungen durchzuf√ºhren. 
# 
# - Wie in Tabelle 4 gezeigt, braucht die Methode Kim und Lee
# sehr lange Zeit aufgrund des iterativen Optimierungsschemas.

# Tabelle 4![OBR11.PNG](./OBR11.PNG)

# **5.5. Auswertung an realen Bildern**<br>
# Da die Ground-Truth-Bilder von realen verschwommenen Bildern nicht verf√ºgbar sind, pr√§sentieren wir nur die visuelle Bewertung
# und Vergleich mit mehreren modernen Methoden f√ºr
# r√§umlich variierende Unsch√§rfeentfernung. 
# Wir vergleichen zun√§chst die
# vorgeschlagene Methode mit der Methode von Sun et.al. zur Motion-Flow-Sch√§tzung. Vier Beispiele sind in Abbildung 7 dargestellt.
# Da das Verfahren von Sun et.al. auf lokalen Patches durchgef√ºhrt wird,
# werden ihre Motion-Flow-komponenten oft falsch eingesch√§tzt, insbesondere wenn das Unsch√§rfemuster in einem kleinen lokalen Bereich subtil oder verwirrend ist, wie beispielsweise in Bereichen mit geringer Beleuchtung oder Texturen. Dank des universellen End-to-End-Mappings k√∂nnen unsere
# Methoden nat√ºrlichere Ergebnisse mit glattem Fluss erzeugen
# und weniger Unordnung. Obwohl wir unser Modell auf Datens√§tzen trainieren
# mit nur sanft variierenden Motion-Flows, verglichen mit
# noMRF Sun et.al. kann unsere Methode bessere Ergebnisse bei Bildern mit
# bewegtem Objekt liefern.
# **Vergleich mit der Methode  Kim et.al.** Bei Kim et. al. verwenden sie
# ein √§hnliches heterogenes Bewegungsunsch√§rfemodell wie unseres und
# sch√§tzen auch den Motion-Flow zum Entsch√§rfen. Weil ihr Code 
# nicht verf√ºgbar ist, f√ºhren wir direkt einen Vergleich mit ihren realen Daten durch. Abbildung 8 zeigt die Ergebnisse an einem Beispiel. Verglichen mit den Ergebnissen von Kim und Lee ist spiegelt unser Bewegungsfluss das komplexe Unsch√§rfemuster genauer wider, und unser
# wiederhergestelltes Bild enth√§lt mehr Details und weniger Artefakte.
# 
# 

# **Text in Stichpunkten:**

# - Da die Ground-Truth-Bilder von realen verschwommenen Bildern nicht verf√ºgbar sind, pr√§sentieren wir **nur die visuelle Bewertung**
# und vergleichen mit mehreren modernen Methoden f√ºr
# r√§umlich variierende Unsch√§rfeentfernung.

# - Da das Verfahren von Sun et.al. auf lokalen Patches durchgef√ºhrt wird,
# werden ihre Motion-Flow-komponenten oft falsch eingesch√§tzt,insbesondere wenn das Unsch√§rfemuster in einem kleinen lokalen Bereich subtil oder verwirrend ist, wie beispielsweise in Bereichen mit geringer Beleuchtung oder Texturen

# - Mit universellen End-to-End-Mappings k√∂nnen unsere
# Methoden nat√ºrlichere Ergebnisse mit glatter Motion-Flow-Sch√§tzung erzeugen
# und weniger Unordnung.

# - Im Vegleich mit noMRF Sun et.al. kann unsere Methode bessere Ergebnisse bei Bildern mit
# bewegtem Objekt liefern

# - Abbildung 8 zeigt die Ergebnisse an einem Beispiel. Verglichen mit den Ergebnissen von Kim und Lee spiegelt unser Motion-Flow das komplexe Unsch√§rfemuster genauer wider, und unser
# wiederhergestelltes Bild enth√§lt mehr Details und weniger Artefakte

# Abbildung 7![OBR9.PNG](./OBR9.PNG)

# Abbildung 8![OBR14.PNG](./OBR14.PNG)

# **Bilder mit Kamerabewegungsunsch√§rfe** Abbildung 9 zeigt ein Beispiel mit Unsch√§rfe, die haupts√§chlich durch die Kamerabewegung verursacht wird.
# Das unscharfe Bild, das von der ungleichm√§√üigen Kamerasch√ºtteln erzeugt wird.
# Die Shake-Deblurring-Methode Whyte et.al. leidet unter starker Unsch√§rfe, da ihr Modell die Unsch√§rfe ignoriert, die durch gro√üe Vorw√§rtsbewegungen verursacht wird
# . Verglichen mit dem Ergebnis von Sun et.al., liefert unsere
# Methode  ein sch√§rferes Ergebnis mit mehr Details und weniger
# Artefakte.
# **Bilder mit Objektbewegungsunsch√§rfe** Wir evaluieren unsere Methode
# auf Bildern mit Objektbewegungsunsch√§rfe. In Abbildung 10
# enth√§lt das Ergebnis von Whyte et.al.  starke Ringing-Artefakte aufgrund der Objektbewegung. Unsere Methode kann mit  
# starker Unsch√§rfe im Hintergrund umgehen und erzeugt ein nat√ºrlicheres
# Bild. Wir vergleichen weiter mit dem segmentierungsbasierten
# Entsch√§rfeverfahren von Pan et.al. auf einem Bild mit gro√üem
# Skalenunsch√§rfe durch bewegte Objekte auf statischem Hintergrund.
# Wie in Abbildung 11 gezeigt, ist das Ergebnis von Sun et.al. aufgrund der Untersch√§tzung des Motion-Flows zu glatt. In dem
# Ergebnis von Pan et.al. einige Details aufgrund der
# Segmentierungsfehler verloren gehen. Unsere vorgeschlagene Methode kann die
# 
# Details auf unscharfem, sich bewegendem Vordergrund wiederherstellen und beh√§lt die Sch√§rfe im
# Hintergrund wie im Original.
# 

# **Text in Stichpunkten:**

# - Abbildung 9 zeigt ein Beispiel mit Unsch√§rfe, die haupts√§chlich durch die Kamerabewegung verursacht wird

# - Whyte et.al. leidet unter starker Unsch√§rfe, da ihr Modell die Unsch√§rfe ignoriert, die durch gro√üe Vorw√§rtsbewegungen verursacht wird

# - Unsere
# Methode liefert ein sch√§rferes Ergebnis mit mehr Details und weniger
# Artefakte

# - Wir evaluieren unsere Methode
# auf Bildern mit Objektbewegungsunsch√§rfe

# - In Abbildung 10
# enth√§lt das Ergebnis von Whyte et.al.  starke Ringing-Artefakte aufgrund der Objektbewegung. Unsere Methode kann mit  
# starker Unsch√§rfe im Hintergrund umgehen und erzeugt ein nat√ºrlicheres
# Bild

# - Wir vergleichen weiter mit dem segmentierungsbasierten
# Entsch√§rfeverfahren von Pan et.al. auf einem Bild mit gro√üem
# Skalenunsch√§rfe durch bewegte Objekte auf statischem Hintergrund

# - Wie in Abbildung 11 gezeigt, ist das Ergebnis von Sun et.al. aufgrund der Untersch√§tzung des Motion-Flows zu glatt.einige Details gehen aufgrund der
# Segmentierungsfehler verloren
# 
# 

# - Unsere Methode kann Details auf unscharfem, sich bewegendem Vordergrund wiederherstellen und beh√§lt die Sch√§rfe im
# Hintergrund wie im Original

# Abbildung 9![OBR12.PNG](./OBR12.PNG)

# Abbildung 10![OBR13.PNG](./OBR13.PNG)

# Abbildung 11![OBR4.PNG](./OBR4.PNG)

# # Fazit

# **6. Fazit**<br>
# In diesem Papier wird ein flexibles und effizientes Deep
# Learning Verfahren zum Sch√§tzen und Entfernen der heterogenen Bewegungsunsch√§rfe vorgestellt. Durch die Darstellung der heterogenen
# Bewegungsunsch√§rfe als pixelweise lineare Bewegungsunsch√§rfe. Die vorgeschlagene
# Methode verwendet ein FCN, um eine dichte Motion-Flow-Map zum Entfernen von Unsch√§rfen zu sch√§tzen
# . Au√üerdem werden automatisch
# Trainingsdaten mit simulierten Motion-Flow-Maps f√ºr das Training
# des FCN generiert. Experimentelle Ergebnisse sowohl von synthetischen als auch realen Daten zeigen die Exzellenz der vorgeschlagenen Methode.
# 
